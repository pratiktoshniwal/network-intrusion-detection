{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Intrusion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is worth 60% of the grade of project 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last four exercises, we use the KDD data set to practice how to create and operate on RDDs. In this project, we will focus on the main purpose of the KDD data set, which is a sample data set for **the Ihird International Knowledge Discovery and Data Mining Tools Competition (KDD cup 99)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KDD cup 99 page describes the motivation of the competition as follows:\n",
    "> Software to detect network intrusions protects a computer network from unauthorized users, including perhaps insiders.  The intrusion detector learning task is to build a predictive model (i.e. a classifier) capable of distinguishing between \"bad\" connections, called intrusions or attacks, and \"good\" normal connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification of the KDD cup 99 mainly targets four types of attacks:\n",
    "- **DOS**: denial-of-service, e.g. syn flood;\n",
    "- **R2L**: unauthorized access from a remote machine, e.g. guessing password;\n",
    "- **U2R**:  unauthorized access to local superuser (root) privileges, e.g., various \"buffer overflow\" attacks;\n",
    "- **probing**: surveillance and other probing, e.g., port scanning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the main target of the KDD cup 99 is **knowledge Discovery** and **Data Mining**, this project will not involve any training and prediction. We will simply use pyspark to explore some well-known rules to extract some useful information from the KDD 99 data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the KDD 99 Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use the reduced dataset (10 percent) provided for the KDD Cup 1999, containing nearly half million network interactions. The file is provided as a Gzip file in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1994</td><td>application_1586837922995_0102</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-66-5.ec2.internal:20888/proxy/application_1586837922995_0102/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-67-234.ec2.internal:8042/node/containerlogs/container_1586837922995_0102_01_000001/han1_yang23\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "data_file = \"/kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created an RDD from the data set. The RDD data sets are structured as the csv format, with each row (i.e., each line) containing the fields of a **network interaction**. The fields in the same row are separated with commas (,). According to <http://kdd.ics.uci.edu/databases/kddcup99/task.html>, each row in the RDD data set contains the following four type of fields:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basic features of individual TCP connections:\n",
    " \n",
    "|feature name  |description                                                  | type       |\n",
    "|--------------|-------------------------------------------------------------|------------|\n",
    "|duration \t   |length (number of seconds) of the connection                 | continuous |\n",
    "|protocol_type |type of the protocol, e.g. tcp, udp, etc.                    | discrete   |\n",
    "|service \t   |network service on the destination, e.g., http, telnet, etc. | discrete   |\n",
    "|src_bytes \t   |number of data bytes from source to destination \t         | continuous |\n",
    "|dst_bytes \t   |number of data bytes from destination to source \t         | continuous |\n",
    "|flag \t       |normal or error status of the connection \t                 | discrete   | \n",
    "|land \t       |1 if connection is from/to the same host/port; 0 otherwise \t | discrete   |\n",
    "|wrong_fragment|number of \"wrong\" fragments \t                             | continuous |\n",
    "|urgent \t   |number of urgent packets \t                                 | continuous |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Content features within a connection suggested by domain knowledge:\n",
    " \n",
    "|feature name      |description                                                  | type       |\n",
    "|------------------|-------------------------------------------------------------|------------|\n",
    "|hot \t           |number of \"hot\" indicators\t                                 |continuous  |\n",
    "|num_failed_logins |number of failed login attempts \t                         |continuous  |\n",
    "|logged_in         |1 if successfully logged in; 0 otherwise \t                 |discrete    |\n",
    "|num_compromised   |number of \"compromised\" conditions \t                         |continuous  |\n",
    "|root_shell        |1 if root shell is obtained; 0 otherwise \t                 |discrete    |\n",
    "|su_attempted      |1 if `su root` command attempted; 0 otherwise \t             |discrete    |\n",
    "|num_root          |number of \"root\" accesses \t                                 |continuous  |\n",
    "|num_file_creations|number of file creation operations \t                         |continuous  |\n",
    "|num_shells        |number of shell prompts \t                                 |continuous  |\n",
    "|num_access_files  |number of operations on access control files \t             |continuous  |\n",
    "|num_outbound_cmds |number of outbound commands in an ftp session \t             |continuous  |\n",
    "|is_hot_login      |1 if the login belongs to the \"hot\" list; 0 otherwise        |discrete    |\n",
    "|is_guest_login    |1 if the login is a \"guest\" login; 0 otherwise               |discrete    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Traffic features computed using a two-second time window:\n",
    " \n",
    "|feature name  |description                                                  | type       |\n",
    "|--------------|-------------------------------------------------------------|------------|\n",
    "|count         |number of connections to the same host as the current connection in the past two seconds|continuous|\n",
    "|-             |Note: The following  features refer to these same-host connections.|      |\t\n",
    "|serror_rate   |% of connections that have \"SYN\" errors \t                 |continuous  |\n",
    "|rerror_rate   |% of connections that have \"REJ\" errors \t                 |continuous  |\n",
    "|same_srv_rate |% of connections to the same service \t                     |continuous  |\n",
    "|diff_srv_rate  |% of connections to different services                      |continuous  |\n",
    "|srv_count      |number of connections to the same service as the current connection in the past two seconds|continuous |\n",
    "|-              |Note: The following features refer to these same-service connections.|    |\t\n",
    "|srv_serror_rate|% of connections that have \"SYN\" errors                     |continuous  |\n",
    "|srv_rerror_rate|% of connections that have \"REJ\" errors                     |continuous  |\n",
    "|srv_diff_host_rate|% of connections to different hosts                      |continuous  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Tags**: classification of network interactions. This field shows the classification of the attack factor for each interactions. Possible values of tags are: back, buffer_overflow, ftp_write, guess_passwd, imap, ipsweep, land, loadmodule, multihop, neptune, nmap, normal, perl, phf, pod, portsweep, rootkit, satan, smurf, spy, teardrop, warezclient, warezmaster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, based on the data set, please list the top 10 attack factors (i.e., tags that are **not** \"normal\") and print in a readable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf.: counts = 27992\n",
      "neptune.: counts = 10660\n",
      "back.: counts = 221\n",
      "satan.: counts = 165\n",
      "ipsweep.: counts = 121\n",
      "portsweep.: counts = 118\n",
      "warezclient.: counts = 103\n",
      "teardrop.: counts = 92\n",
      "pod.: counts = 25\n",
      "nmap.: counts = 19\n",
      "Evaluation has taken 3.702 secs"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "\n",
    "# First RDD:  normal_interactions = interaction with tag = \"normal.\"\n",
    "normal_interactions = raw_data.filter(lambda x: 'normal.' in x)\n",
    "\n",
    "# Second RDD: attack_interactions = interaction with tag != \"normal.\"\n",
    "attack_interactions = raw_data.subtract(normal_interactions)\n",
    "\n",
    "# third RDD:  attack_tag_counts = interaction count of each tag which is not \"normal\".\n",
    "attack_tag_interactions = attack_interactions.map(lambda x: (x.split(',')[-1], x))\n",
    "attack_tag_counts = attack_tag_interactions.countByKey()\n",
    "\n",
    "# fourth RDD: attack_tag_counts_sorted = sorted list of tags with their interaction counts.\n",
    "attack_tag_counts_sorted = [(tag, attack_tag_counts[tag]) for tag in sorted(attack_tag_counts, key=attack_tag_counts.get, reverse=True)]\n",
    "\n",
    "# result:     attack_tag_counts_top10 = top 10 attack tags and their interaction counts.\n",
    "attack_tag_counts_top10 = attack_tag_counts_sorted[:10]\n",
    "\n",
    "tt = time() - t0\n",
    "\n",
    "for (tag, count) in attack_tag_counts_top10:\n",
    "    print tag + \": counts = \" + str(count)\n",
    "    \n",
    "print \"Evaluation has taken {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Denial of Service (DDOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KDD 99 data set has defined six primary types of DOS attacks: **back**, **land**, **neptune**, **pod**, **smurf**, **teardrop**. Without further details of these DOS attacks, can you identified the attacks which are **most distributed**, as well as the attacks which are **most correlated with SYN errors**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, filter the network interactions in the raw data to contain only these six types of DOS attacks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod.: counts = 25\n",
      "neptune.: counts = 10660\n",
      "back.: counts = 221\n",
      "teardrop.: counts = 92\n",
      "land.: counts = 4\n",
      "smurf.: counts = 27992\n",
      "Filtering DOS attacks has taken 0.419 secs"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Filter the network interactions to include only DOS attacks\n",
    "dos_attacks = ['back.', 'land.', 'neptune.', 'pod.', 'smurf.', 'teardrop.']\n",
    "dos_attack_data = attack_tag_interactions.filter(lambda x: x[0] in dos_attacks)\n",
    "\n",
    "# Calculate the count of DOS interactions for each tag\n",
    "dos_attack_counts = list(dos_attack_data.countByKey().items())\n",
    "\n",
    "tt = time() - t0\n",
    "\n",
    "for (tag, count) in dos_attack_counts:\n",
    "    print tag + \": counts = \" + str(count)\n",
    "\n",
    "print \"Filtering DOS attacks has taken {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, sort the DOS attacks to show, from the highest to the lowest, **the average numbers of connections** within the last 2 seconds (in regards to each host) involved in each DOS attack. Hint: use the mean values of the field `count` to determine the **most distributed** DOS attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smurf.: degree = 507.182695056\n",
      "neptune.: degree = 188.613602251\n",
      "teardrop.: degree = 62.8586956522\n",
      "back.: degree = 3.41628959276\n",
      "pod.: degree = 2.96\n",
      "land.: degree = 1.0\n",
      "Sorting DOS attacks by degree of distribution has taken 0.677 secs"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Calculate the mean values of distribution degrees (# of connections involved with the interaction)\n",
    "dos_attack_degree_data = dos_attack_data.map(lambda x: (x[0], float(x[1].split(',')[22])))\n",
    "dos_attack_degrees = dos_attack_degree_data.combineByKey(\n",
    "    (lambda x: (x, 1)),\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)),\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1]))\n",
    ")\n",
    "dos_attack_degrees = dos_attack_degrees.map(lambda (key, value): (key, value[0]/value[1]))\n",
    "\n",
    "sorted_dos_attack_degrees = sorted(dos_attack_degrees.collect(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "tt = time() - t0\n",
    "\n",
    "for (tag, degree) in sorted_dos_attack_degrees:\n",
    "    print tag + \": degree = \" + str(degree)\n",
    "\n",
    "print \"Sorting DOS attacks by degree of distribution has taken {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sort the DOS attacks show, from the highest to the lowest, **the average numbers of SYN errors** within the last 2 seconds (in regards to each host) involved in each DOS attack. Hint: approximate the numbers of SYN errors by multiplying `count` with `serror_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neptune.: degree = 153.714626642\n",
      "teardrop.: degree = 16.3267391304\n",
      "land.: degree = 1.0\n",
      "back.: degree = 0.0\n",
      "pod.: degree = 0.0\n",
      "smurf.: degree = 0.0\n",
      "Sorting DOS attacks by correlation with SYN flooding has taken 0.816 secs"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "# Calculate the average numbers of connections with SYN errors (# of connections involved with the interaction)\n",
    "dos_attack_syn_error_data = dos_attack_data.map(lambda x: (x[0], int(x[1].split(',')[22]) * float(x[1].split(',')[24])))\n",
    "\n",
    "dos_attack_syn_error_sum = dos_attack_syn_error_data.combineByKey(\n",
    "    (lambda x: (x, 1)),\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)),\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1]))\n",
    ")\n",
    "dos_attack_syn_error_counts = dos_attack_syn_error_sum.map(lambda (key, value): (key, value[0]/value[1]))\n",
    "\n",
    "sorted_dos_attack_syn_error_counts = sorted(dos_attack_syn_error_counts.collect(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "tt = time() - t0\n",
    "\n",
    "for (tag, count) in sorted_dos_attack_syn_error_counts:\n",
    "    print tag + \": degree = \" + str(count)\n",
    "\n",
    "print \"Sorting DOS attacks by correlation with SYN flooding has taken {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute-force Login Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brute-force login attack relies on the attacker continuously re-attempting failed login to a host until being able to log into the host and eventually to perform `su` to become the root user. A host often has defense mechanisms such as ASLR (Address Space Layout Randomization) to reduce the probability for an attack to successfully perform `su` following successful login."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will calculate the **the average number of failed login attempt** before successful login (i.e., `logged_in == 1`), and **the average number of failed login attempt** before successful `su` to gain the root shell (i.e., `root_shell == 1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of attempts before successful login: 0.457846070166\n",
      "Average number of attempts before successful su: 14.0\n",
      "Calculating the difficulty of brute-force login attacks has taken 3.669 secs"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "csv_data = raw_data.map(lambda x: x.split(','))\n",
    "\n",
    "# Calculate the average numbers of failed login attempt before successful login\n",
    "logged_in_data = csv_data.filter(lambda x: x[11] == '1')\n",
    "logged_in_failed_times = logged_in_data.map(lambda x: float(x[10]))\n",
    "attempt_before_login = logged_in_failed_times.reduce(lambda x, y: x + y)\n",
    "\n",
    "average_attempt_before_login = attempt_before_login / logged_in_data.count()\n",
    "\n",
    "# Calculate the average numbers of failed login attempt before successful `su` to gain the root shell\n",
    "root_shell_data = csv_data.filter(lambda x: x[13] == '1')\n",
    "root_shell_failed_times = root_shell_data.map(lambda x: float(x[10]))\n",
    "attempt_before_su = root_shell_failed_times.reduce(lambda x, y: x + y)\n",
    "\n",
    "average_attempt_before_su = attempt_before_su / root_shell_data.count()\n",
    "\n",
    "tt = time() - t0\n",
    "\n",
    "\n",
    "print \"Average number of attempts before successful login: {}\".format(average_attempt_before_login)\n",
    "print \"Average number of attempts before successful su: {}\".format(average_attempt_before_su)\n",
    "\n",
    "print \"Calculating the difficulty of brute-force login attacks has taken {} secs\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready for grading."
     ]
    }
   ],
   "source": [
    "# If you have finished this notebook and are ready to submit, please uncomment and\n",
    "# execute the following code to let the TA know.\n",
    "print \"Ready for grading.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
